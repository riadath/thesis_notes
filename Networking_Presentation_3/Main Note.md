##### ChatGPT Chat : https://chat.openai.com/share/b0809537-201d-4d1a-a077-171fd751135e

# Abstract

#### <u>Dynamic Voronoi Partitions</u>
Dynamic Voronoi is a technique used in the proposed cooperative exploration approach for multiple mobile robots to minimize duplicated exploration areas by assigning different target locations to individual robots. [It is developed using dynamic Voronoi partitions](http://wiki.ros.org/dynamicvoronoi) [1](http://wiki.ros.org/dynamicvoronoi).
The Voronoi diagram is a partition of a plane into regions close to each of a given set of objects. [Voronoi diagram - Wikipedia](https://en.wikipedia.org/wiki/Voronoi_diagram#:~:text=In%20mathematics%2C%20a%20Voronoi%20diagram,%2C%20sites%2C%20or%20generators).

# Introduction

### <u>Problems</u>
Most exploration methods were designed for single robots and had limitations in exploring large unknown areas.

### <u>Contributions of this paper</u>
1. A Voronoi-based exploration strategy is developed to coordinate a multi-robot team effectively in exploring an unknown area
2. A collision avoidance algorithm with deep reinforcement learning is proposed to navigate the robot to the target
3. The feasibility of the proposed cooperative exploration strategy is validated by real-world experiments using wheeled mobile robots

#### <u>Buzz words and Abbreviations</u>

1. **FEP (Frontier Exploration Planning)**: FEP is a global exploration planner that helps a robot identify unexplored frontiers in an environment. It focuses on selecting areas on the frontier of explored space for further exploration.
    
2. **NBVP (Next-Best-View Planning)**: NBVP is a local exploration planning technique that determines the best next viewpoint for a robot to gather information. It's often used in conjunction with FEP to guide a robot in exploring its immediate surroundings.
    
3. **Heuristic Information Gain-Based NBVP**: This approach combines the principles of NBVP with heuristics based on information gain. It aims to find the next best view for a robot while considering factors like information content and efficiency, improving exploration in 3-D unknown environments.



# Technical Background

### The Voronoi Cell $Vor(R_i)$ is defined by
$Vor(R_i)=\{q \in Q \ \ | \ \ \lVert q-p_i \rVert \le \lVert q-p_j \rVert,\forall R_j \in N_{R_i}\}$

### Hierarchical control architecture

![[Pasted image 20231029213948.png | 500x420]]

### Collision Avoidance Problem Formulation
### $v_t = f(l_t,p_t,v_{t-1})$
- where $l_t$ is the laser range data at time step t 
- $p_t$ stands for the relative position of the target,
- $v_{t-1}$ denotes the velocity command at the last time step

### Coordination Algorithm for Explorer Robots
we will define the utility function for frontier points $k$ assigned to the robot $R_i$ as
![[Pasted image 20231031035846.png | 300x60]]

## <u>Deep Reinforcement Learning Setup</u>

### Buzz Words and Abbreviations

**DDPG*** : DDPG stands for Deep Deterministic Policy Gradient. DDPG combines ideas from both policy-based methods (actor) and value-based methods (critic) to learn and optimize policies for agents operating in environments.
**PER** : Prioritized Experience Replay (PER) is a technique used in the context of reinforcement learning. It enhances the learning process by giving higher priority to certain experiences during the training of an agent
### Reward Space
##### Reward Function : $r=r_d+r_{cl}+r_{av}+r_{lv}$
$r_d$ represents the distance reward, $r_{cl}$ describes the safety clearance reward, $r_{av}$ denotes the angular velocity reward, and $r_{lv}$ is the linear velocity reward.

### Network Structure
- The input to the neural network is the concatenation vector of rangefinder data (24-dimensional vector)
- The input layer is connected with three dense layers with each layer having 512 nodes
- The actor network finally generates the<mark style="background: #BBFABBA6;"> linear velocity</mark> command through a sigmoid function and produces the <mark style="background: #BBFABBA6;">angular velocity</mark> using a hyperbolic tangent function.
- These two velocity commands are finally concatenated into the <mark style="background: #BBFABBA6;">action state</mark>
![[Pasted image 20231102020440.png | 400x150]]
- The state data of the robot (28-dimensional vector) is also used as the input of the critic network and is processed by three dense layers and each layer has 512 nodes
- . The action input is merged with the second layer, and the Q-value is finally generated by a linear activation function
![[Pasted image 20231102022021.png | 400x200]]

**We integrate DDPG with Prioritized Experience Replay (PER) algorithm using human demonstration data. In this research, human demonstration data $R_{demo}$ is recorded before training the networks**

### Algorithm 2
![[Pasted image 20231102022349.png]]

## Explanation

### Terms Explanation
- **Replay Buffer**: It serves as a data storage mechanism
	- Stores (Current state, Action state, Reward, Next state)
	-  Instead of updating the agent's policy or Q-values based on experiences immediately as they occur, experiences are collected and stored in the replay buffer.
- **Sampling Priority**: Refers to the priority assigned to each transition within the Prioritized Experience Replay (PER) framework
	- Sampling priority is used to determine the likelihood of a particular transition being selected for training when sampling from the replay buffer. 
	- The higher the priority, the more likely it is to be selected
	- $p(i) = \delta_i^2 + \lambda |\nabla_a \cdot Q(s_i, a_i | \theta_Q)|^2 + \tau_p + \tau_D$
	-  $\delta$ is the Temporal Difference (TD): the difference between the estimated value of a state or state-action pair and the expected value at a later time step 
	- $|\Delta(a) \cdot Q(s_i, a_i | \theta_Q)|^2$ is the actor's loss
	- $\tau_p$ and $\tau_d$ are user-defined constant value
- **Sampling Probability**: Likelihood of a transition being selected $P(i) = \frac{p_i^\alpha}{\sum_{k}p_k^\alpha}$
- **Target Value of Critic Network** ($y_i$) is the target value of the critic network
	- ![[Pasted image 20231102042219.png]]
	- Target networks have the same architecture as the main networks but are not updated as frequently
	- The target network is updated periodically to prevent the overestimation of Q-values
	- It's calculated using the Bellman equation
	- ![[Pasted image 20231102042316.png]]
	- $\alpha$ is the learning rate, $r$ is the reward. $\gamma$ is the discount factor
- **Weighted updates ($\omega_i$): indicates the importance of that transition when sampling experiences from the replay buffer during the training process
- ![[Pasted image 20231102043354.png]]
	- We're taking the squared mean error.
	- Which measures the difference between the predicted Q-values $Q(s,a|\theta_Q)$ and target Q-values $y_i$
	- The goal is to make the predicted Q-values as close as possible to the target Q-values.
- **Update the actor policy using the sampled policy gradient**
- ![[Pasted image 20231102044833.png]]
	- $J$ : The gradient of the expected return. Represents the expected sum of rewards an agent can achieve when following a particular policy in an environment over time
	- $\nabla_a Q(a|\theta_Q)$ The gradient of the Q-value with respect to action
	- $\nabla_{\theta_\mu} \mu(a|\theta_Q)$ The gradient of the actor's policy concerning its parameters $\theta_\mu$
	- The update to the actor's policy ($\theta_\mu$​) is performed in the direction of the gradient, which encourages the actor to select actions that lead to higher expected returns.
		
### Algo 2 Steps
- Initialize everything
- Initialize replay buffer.  
- For each step in $for(1,M)$ : 
	- Randomly choose angular velocity ($v_a$) and linear velocity ($v_l$). Receive initial observation state $s_1$
	- For each step in $for(1,T)$ : 
		- Select an action ($a_t$) according to the current policy and exploration noise
		- Execute the action, calculate the reward, and observe the new state.
		- Calculate the priority ($p_i$) for the current transition.
		- Store the transition data (state, action, reward, next state, and priority) in the replay buffer.
		- Sample a minibatch of transitions from the replay buffer based on their priority values $P(i)$
		- Update the critic network by minimizing the loss using the sampled transitions.
		- Update the actor policy using the policy gradient concerning the critic network.
		- Update the target networks
